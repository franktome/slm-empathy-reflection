{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a39f153",
   "metadata": {},
   "source": [
    "# Stage 0 - externel emotion pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea0de3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_emotions: 28\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['labels', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 43410\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['labels', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 5426\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['labels', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 5427\n",
      "    })\n",
      "})\n",
      "{'labels': tensor(27), 'input_ids': tensor([   0, 2387, 5548,  689,   16,  932,   38,  399,   75,   33,    7, 7142,\n",
      "        2185,    4,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\yunhs\\AppData\\Local\\Temp\\ipykernel_13544\\2553752735.py:98: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_goemo = Trainer(\n",
      "c:\\Users\\yunhs\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\transformers\\utils\\generic.py:255: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5428' max='5428' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5428/5428 19:38, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.488600</td>\n",
       "      <td>1.397162</td>\n",
       "      <td>0.578879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.273400</td>\n",
       "      <td>1.344146</td>\n",
       "      <td>0.592518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yunhs\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\transformers\\utils\\generic.py:255: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved pretrained emotion encoder to: ./goemo_out\\pretrained_emotion_encoder.pt\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    ")\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "\n",
    "# 로컬 경로\n",
    "SAVE_DIR = \"./goemo_out\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "SAVE_ENCODER_PATH = os.path.join(SAVE_DIR, \"pretrained_emotion_encoder.pt\")\n",
    "\n",
    "# 1) GoEmotions 로드\n",
    "goemo = load_dataset(\"go_emotions\")   # train / validation / test\n",
    "\n",
    "# 클래스 개수 (28개)\n",
    "num_emotions = len(goemo[\"train\"].features[\"labels\"].feature.names)\n",
    "print(\"num_emotions:\", num_emotions)\n",
    "\n",
    "# 2) tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 3) 전처리: 단일 라벨로 변환 + 토크나이즈\n",
    "def preprocess(batch):\n",
    "    enc = tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "    )\n",
    "\n",
    "    single_labels = []\n",
    "    for label_list in batch[\"labels\"]:\n",
    "        if len(label_list) > 0:\n",
    "            # 여러 개 있으면 첫 번째 라벨만 사용 (대표 감정)\n",
    "            single_labels.append(label_list[0])\n",
    "        else:\n",
    "            # 라벨이 비어있는 경우 0번 클래스(기본값)으로\n",
    "            single_labels.append(0)\n",
    "\n",
    "    enc[\"labels\"] = single_labels\n",
    "    return enc\n",
    "\n",
    "goemo_proc = goemo.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=goemo[\"train\"].column_names,  # text, labels, id 등 제거\n",
    ")\n",
    "\n",
    "goemo_proc.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    ")\n",
    "\n",
    "print(goemo_proc)\n",
    "print(goemo_proc[\"train\"][0])\n",
    "\n",
    "# 4) 모델 준비: 다중 클래스 분류 (CrossEntropyLoss)\n",
    "model_emo = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_emotions,\n",
    ")\n",
    "\n",
    "# 5) metric (일반 accuracy)\n",
    "acc_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = acc_metric.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
    "    return {\"accuracy\": acc}\n",
    "\n",
    "# 6) TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=SAVE_DIR,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=2,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    logging_steps=200,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# 7) Trainer\n",
    "trainer_goemo = Trainer(\n",
    "    model=model_emo,\n",
    "    args=training_args,\n",
    "    train_dataset=goemo_proc[\"train\"],\n",
    "    eval_dataset=goemo_proc[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 8) 학습\n",
    "trainer_goemo.train()\n",
    "\n",
    "# 9) encoder만 저장 (roberta-base 기준)\n",
    "torch.save(model_emo.roberta.state_dict(), SAVE_ENCODER_PATH)\n",
    "print(\"✅ Saved pretrained emotion encoder to:\", SAVE_ENCODER_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb81fcb7",
   "metadata": {},
   "source": [
    "# Stage 1 — Multi-View Dataset 구축 (context-only + ctx+rsp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fffd301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['context', 'response', 'label', 'conv_id', 'emotion', 'type', 'final_hybrid', 'llm_15', 'rule_15', 'seed_15', 'emotion_id'],\n",
      "        num_rows: 5076\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['context', 'response', 'label', 'conv_id', 'emotion', 'type', 'final_hybrid', 'llm_15', 'rule_15', 'seed_15', 'emotion_id'],\n",
      "        num_rows: 627\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['context', 'response', 'label', 'conv_id', 'emotion', 'type', 'final_hybrid', 'llm_15', 'rule_15', 'seed_15', 'emotion_id'],\n",
      "        num_rows: 669\n",
      "    })\n",
      "})\n",
      "{'context': \"I felt guilty when I was driving home one night and a person tried to fly into my lane, and didn't see me. I honked and they swerved back into their lane, slammed on their brakes, and hit the water cones. [SEP] Yeah about 10 years ago I had a horrifying experience. It was 100% their fault but they hit the water barrels and survived. They had no injuries but they almost ran me off the road.\", 'response': 'Did you suffer any injuries?', 'label': 3.640000104904175, 'conv_id': 'hit:0_conv:0', 'emotion': 'guilty', 'type': 'positive', 'final_hybrid': 3.640000104904175, 'llm_15': 2.0, 'rule_15': 1.7999999523162842, 'seed_15': 5.0, 'emotion_id': 18}\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids_ctx', 'attention_mask_ctx', 'input_ids_full', 'attention_mask_full', 'labels', 'emotion_labels'],\n",
      "        num_rows: 5076\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids_ctx', 'attention_mask_ctx', 'input_ids_full', 'attention_mask_full', 'labels', 'emotion_labels'],\n",
      "        num_rows: 627\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids_ctx', 'attention_mask_ctx', 'input_ids_full', 'attention_mask_full', 'labels', 'emotion_labels'],\n",
      "        num_rows: 669\n",
      "    })\n",
      "})\n",
      "{'input_ids_ctx': tensor([    0,  7164,  1000,    35,    38,  1299,  2181,    77,    38,    21,\n",
      "         1428,   184,    65,   363,     8,    10,   621,  1381,     7,  3598,\n",
      "           88,   127,  6625,     6,     8,   399,    75,   192,   162,     4,\n",
      "           38, 11006, 10916,     8,    51,  3514, 13539,   124,    88,    49,\n",
      "         6625,     6,  9597,    15,    49, 17690,     6,     8,   478,     5,\n",
      "          514, 37407,     4,   646,  3388,   510,   742,  8976,    59,   158,\n",
      "          107,   536,    38,    56,    10, 28242,   676,     4,    85,    21,\n",
      "          727,   207,    49,  7684,    53,    51,   478,     5,   514,  6745,\n",
      "            8,  5601,     4,   252,    56,   117,  1746,    53,    51,   818,\n",
      "         2075,   162,   160,     5,   921,     4,     2,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1]), 'attention_mask_ctx': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'input_ids_full': tensor([    0,  7164,  1000,    35,    38,  1299,  2181,    77,    38,    21,\n",
      "         1428,   184,    65,   363,     8,    10,   621,  1381,     7,  3598,\n",
      "           88,   127,  6625,     6,     8,   399,    75,   192,   162,     4,\n",
      "           38, 11006, 10916,     8,    51,  3514, 13539,   124,    88,    49,\n",
      "         6625,     6,  9597,    15,    49, 17690,     6,     8,   478,     5,\n",
      "          514, 37407,     4,   646,  3388,   510,   742,  8976,    59,   158,\n",
      "          107,   536,    38,    56,    10, 28242,   676,     4,    85,    21,\n",
      "          727,   207,    49,  7684,    53,    51,   478,     5,   514,  6745,\n",
      "            8,  5601,     4,   252,    56,   117,  1746,    53,    51,   818,\n",
      "         2075,   162,   160,     5,   921,     4,   646,  3388,   510,   742,\n",
      "          248,  4186,    35,  6553,    47,  6297,   143,  1746,   116,     2,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1]), 'attention_mask_full': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor(3.6400), 'emotion_labels': tensor(18)}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# ===== 경로 설정 =====\n",
    "RAW_ED_PATH = \"empathy_dataset/hf_raw_with_emo\"  # 네가 저장한 경로로 수정\n",
    "SAVE_ENCODER_PATH = \"./goemo_out/pretrained_emotion_encoder.pt\"\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "\n",
    "# 1) ED 데이터 로드\n",
    "ds = load_from_disk(RAW_ED_PATH)\n",
    "print(ds)\n",
    "\n",
    "# 확인: train[0]에 'context', 'response', 'label', 'emotion_id' 등이 있어야 함\n",
    "print(ds[\"train\"][0])\n",
    "\n",
    "# 2) multi-view 텍스트 생성\n",
    "def build_multiview(example):\n",
    "    ctx = example[\"context\"]\n",
    "    rsp = example[\"response\"]\n",
    "\n",
    "    # 감정 분류용 view: context-only\n",
    "    example[\"input_ctx\"] = \"CTX: \" + ctx\n",
    "\n",
    "    # 공감 회귀용 view: context + response\n",
    "    example[\"input_full\"] = f\"CTX: {ctx} [SEP] RSP: {rsp}\"\n",
    "    return example\n",
    "\n",
    "ds_mv = ds.map(build_multiview)\n",
    "\n",
    "# 3) 토크나이저 준비\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "MAX_LEN = 256\n",
    "\n",
    "def tokenize_views(batch):\n",
    "    enc_ctx = tokenizer(\n",
    "        batch[\"input_ctx\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "    enc_full = tokenizer(\n",
    "        batch[\"input_full\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_ids_ctx\": enc_ctx[\"input_ids\"],\n",
    "        \"attention_mask_ctx\": enc_ctx[\"attention_mask\"],\n",
    "        \"input_ids_full\": enc_full[\"input_ids\"],\n",
    "        \"attention_mask_full\": enc_full[\"attention_mask\"],\n",
    "        # 공감 회귀 타깃: label(=final_hybrid)\n",
    "        \"labels\": batch[\"label\"],\n",
    "        # 감정 분류 타깃: emotion_id\n",
    "        \"emotion_labels\": batch[\"emotion_id\"],\n",
    "    }\n",
    "\n",
    "# 4) 원래 컬럼들은 제거하고 multi-view 텐서만 남기기\n",
    "cols_to_remove = ds_mv[\"train\"].column_names\n",
    "ds_tokenized = ds_mv.map(\n",
    "    tokenize_views,\n",
    "    batched=True,\n",
    "    remove_columns=cols_to_remove,\n",
    ")\n",
    "\n",
    "# 5) torch 텐서 포맷 설정\n",
    "for split in ds_tokenized.keys():\n",
    "    ds_tokenized[split].set_format(\n",
    "        type=\"torch\",\n",
    "        columns=[\n",
    "            \"input_ids_ctx\", \"attention_mask_ctx\",\n",
    "            \"input_ids_full\", \"attention_mask_full\",\n",
    "            \"labels\", \"emotion_labels\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "print(ds_tokenized)\n",
    "print(ds_tokenized[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6784faff",
   "metadata": {},
   "source": [
    "# 공유 encoder + multi-view cascade 모델 정의\n",
    "- emotion branch: input_ctx만 encoder에 넣고 emtion logits 계산\n",
    "- Empathy branch: input_full을 encoder에 넣고 CLS + emotion_logits를 concat해서 최종 공감 점수 회귀\n",
    "- pre-trained encoder로 초기화: Stage 0에서 만든 pretrained_emotion_encoder.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1c29d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_emotions: 32\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "num_emotions = int(max(ds[\"train\"][\"emotion_id\"])) + 1\n",
    "print(\"num_emotions:\", num_emotions)\n",
    "\n",
    "class MultiViewCascadeModel(nn.Module):\n",
    "    def __init__(self, encoder_name, num_emotions, pretrained_encoder_path=None):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(encoder_name)\n",
    "        hidden = self.encoder.config.hidden_size\n",
    "\n",
    "        # Stage0에서 학습한 encoder 가중치 로드\n",
    "        if pretrained_encoder_path is not None:\n",
    "            print(\">> Loading pretrained encoder from:\", pretrained_encoder_path)\n",
    "            state = torch.load(pretrained_encoder_path, map_location=\"cpu\")\n",
    "            self.encoder.load_state_dict(state, strict=False)\n",
    "\n",
    "        # Emotion head (context-only)\n",
    "        self.emo_head = nn.Linear(hidden, num_emotions)\n",
    "\n",
    "        # Empathy regression head (CLS_full + emo_logits)\n",
    "        self.reg_head = nn.Sequential(\n",
    "            nn.Linear(hidden + num_emotions, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden, 1),\n",
    "        )\n",
    "\n",
    "        self.loss_reg = nn.SmoothL1Loss()\n",
    "        self.loss_emo = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids_ctx=None,\n",
    "        attention_mask_ctx=None,\n",
    "        input_ids_full=None,\n",
    "        attention_mask_full=None,\n",
    "        labels=None,\n",
    "        emotion_labels=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # 1) Emotion branch: context-only\n",
    "        out_ctx = self.encoder(\n",
    "            input_ids=input_ids_ctx,\n",
    "            attention_mask=attention_mask_ctx,\n",
    "        )\n",
    "        cls_ctx = out_ctx.last_hidden_state[:, 0]          # [B, H]\n",
    "        emo_logits = self.emo_head(cls_ctx)                # [B, C]\n",
    "\n",
    "        # 2) Empathy branch: context + response\n",
    "        out_full = self.encoder(\n",
    "            input_ids=input_ids_full,\n",
    "            attention_mask=attention_mask_full,\n",
    "        )\n",
    "        cls_full = out_full.last_hidden_state[:, 0]        # [B, H]\n",
    "\n",
    "        # concat [CLS_full ; emo_logits] → 회귀\n",
    "        feat = torch.cat([cls_full, emo_logits], dim=-1)   # [B, H + C]\n",
    "        reg_score = self.reg_head(feat).squeeze(-1)        # [B]\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None and emotion_labels is not None:\n",
    "            # 공감 회귀 손실\n",
    "            loss_r = self.loss_reg(reg_score, labels.float())\n",
    "            # 감정 분류 손실\n",
    "            loss_e = self.loss_emo(emo_logits, emotion_labels.long())\n",
    "\n",
    "            lambda_emo = 0.1\n",
    "            loss = loss_r + lambda_emo * loss_e\n",
    "\n",
    "        # logits: [B, 1 + num_emotions]로 묶어서 Trainer에 넘김\n",
    "        logits_concat = torch.cat(\n",
    "            [reg_score.unsqueeze(-1), emo_logits.detach()],\n",
    "            dim=-1\n",
    "        )  # [B, 1 + C]\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": logits_concat,          # compute_metrics에서 풀어씀\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a165ecaf",
   "metadata": {},
   "source": [
    "# 3. Trainer용 Metrics (MAE + Spearman + emotion accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c3328f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "mae_metric = evaluate.load(\"mae\")\n",
    "acc_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics_multi(eval_pred):\n",
    "    # eval_pred = (logits, labels)\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    # labels: tuple → (regression labels, emotion_labels)\n",
    "    reg_true = labels[0]          # shape: (N,)\n",
    "    emo_true = labels[1]          # shape: (N,)\n",
    "\n",
    "    # logits: np.ndarray → shape: (N, 1 + num_emotions)\n",
    "    reg_pred = logits[:, 0]       # 공감 점수 예측 (N,)\n",
    "    emo_logits = logits[:, 1:]    # 감정 로짓 (N, C)\n",
    "    emo_pred = np.argmax(emo_logits, axis=-1)  # (N,)\n",
    "\n",
    "    # MAE\n",
    "    mae = float(np.mean(np.abs(reg_pred - reg_true)))\n",
    "\n",
    "    # Spearman 상관계수\n",
    "    spearman_corr = spearmanr(reg_true, reg_pred).correlation\n",
    "\n",
    "    # 감정 분류 정확도\n",
    "    emo_acc = float((emo_pred == emo_true).mean())\n",
    "\n",
    "    return {\n",
    "        \"eval_mae\": mae,\n",
    "        \"eval_spearman\": spearman_corr if spearman_corr is not None else 0.0,\n",
    "        \"eval_emotion_accuracy\": emo_acc,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ea1d11",
   "metadata": {},
   "source": [
    "# 4. TrainingArguments & Trainer설정 + 학습&평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23578cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\yunhs\\AppData\\Local\\Temp\\ipykernel_13544\\3312283143.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(pretrained_encoder_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Loading pretrained encoder from: ./goemo_out/pretrained_emotion_encoder.pt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2540' max='2540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2540/2540 11:44, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mae</th>\n",
       "      <th>Spearman</th>\n",
       "      <th>Emotion Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.680900</td>\n",
       "      <td>0.629307</td>\n",
       "      <td>0.728914</td>\n",
       "      <td>0.209364</td>\n",
       "      <td>0.248804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.407700</td>\n",
       "      <td>0.367283</td>\n",
       "      <td>0.403557</td>\n",
       "      <td>0.744814</td>\n",
       "      <td>0.411483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.210100</td>\n",
       "      <td>0.315620</td>\n",
       "      <td>0.309449</td>\n",
       "      <td>0.787719</td>\n",
       "      <td>0.459330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.169300</td>\n",
       "      <td>0.314966</td>\n",
       "      <td>0.301260</td>\n",
       "      <td>0.779292</td>\n",
       "      <td>0.492823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Train result ==\n",
      "TrainOutput(global_step=2540, training_loss=0.39646767631290464, metrics={'train_runtime': 705.0332, 'train_samples_per_second': 28.799, 'train_steps_per_second': 3.603, 'total_flos': 0.0, 'train_loss': 0.39646767631290464, 'epoch': 4.0})\n",
      "Best checkpoint: ./mv_cascade_ed_out\\checkpoint-2540\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='82' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Validation metrics ==\n",
      "eval_mae: 0.3013\n",
      "eval_spearman: 0.7793\n",
      "eval_emotion_accuracy: 0.4928\n",
      "eval_loss: 0.3150\n",
      "eval_runtime: 4.5012\n",
      "eval_samples_per_second: 139.2950\n",
      "eval_steps_per_second: 8.8860\n",
      "epoch: 4.0000\n",
      "== Test metrics ==\n",
      "eval_mae: 0.3318\n",
      "eval_spearman: 0.7541\n",
      "eval_emotion_accuracy: 0.4081\n",
      "eval_loss: 0.3696\n",
      "eval_runtime: 2.1090\n",
      "eval_samples_per_second: 317.2070\n",
      "eval_steps_per_second: 19.9140\n",
      "epoch: 4.0000\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "output_dir = \"./mv_cascade_ed_out\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"mae\",   # 공감 회귀 기준으로 best 선택\n",
    "    greater_is_better=False,\n",
    "    logging_steps=50,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    "    # 두 개의 label을 Trainer가 넘겨주게 지정\n",
    "    label_names=[\"labels\", \"emotion_labels\"],\n",
    ")\n",
    "\n",
    "set_seed(17)\n",
    "\n",
    "model = MultiViewCascadeModel(\n",
    "    encoder_name=MODEL_NAME,\n",
    "    num_emotions=num_emotions,\n",
    "    pretrained_encoder_path=SAVE_ENCODER_PATH,   # Stage0에서 만든 encoder\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_tokenized[\"train\"],\n",
    "    eval_dataset=ds_tokenized[\"validation\"],\n",
    "    compute_metrics=compute_metrics_multi,\n",
    ")\n",
    "\n",
    "# 학습\n",
    "train_result = trainer.train()\n",
    "print(\"== Train result ==\")\n",
    "print(train_result)\n",
    "\n",
    "print(\"Best checkpoint:\", trainer.state.best_model_checkpoint)\n",
    "\n",
    "# Validation 성능\n",
    "val_metrics = trainer.evaluate(ds_tokenized[\"validation\"])\n",
    "print(\"== Validation metrics ==\")\n",
    "for k, v in val_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
    "\n",
    "# Test 성능\n",
    "test_metrics = trainer.evaluate(ds_tokenized[\"test\"])\n",
    "print(\"== Test metrics ==\")\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a0aadc",
   "metadata": {},
   "source": [
    "## MultiView 토크나이즈 데이터셋 재생성 + 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "604d26ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['context', 'response', 'label', 'conv_id', 'emotion', 'type', 'final_hybrid', 'llm_15', 'rule_15', 'seed_15', 'emotion_id'],\n",
      "        num_rows: 5076\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['context', 'response', 'label', 'conv_id', 'emotion', 'type', 'final_hybrid', 'llm_15', 'rule_15', 'seed_15', 'emotion_id'],\n",
      "        num_rows: 627\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['context', 'response', 'label', 'conv_id', 'emotion', 'type', 'final_hybrid', 'llm_15', 'rule_15', 'seed_15', 'emotion_id'],\n",
      "        num_rows: 669\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids_ctx', 'attention_mask_ctx', 'input_ids_full', 'attention_mask_full', 'labels', 'emotion_labels'],\n",
      "        num_rows: 5076\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids_ctx', 'attention_mask_ctx', 'input_ids_full', 'attention_mask_full', 'labels', 'emotion_labels'],\n",
      "        num_rows: 627\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids_ctx', 'attention_mask_ctx', 'input_ids_full', 'attention_mask_full', 'labels', 'emotion_labels'],\n",
      "        num_rows: 669\n",
      "    })\n",
      "})\n",
      "{'input_ids_ctx': tensor([    0,  7164,  1000,    35,    38,  1299,  2181,    77,    38,    21,\n",
      "         1428,   184,    65,   363,     8,    10,   621,  1381,     7,  3598,\n",
      "           88,   127,  6625,     6,     8,   399,    75,   192,   162,     4,\n",
      "           38, 11006, 10916,     8,    51,  3514, 13539,   124,    88,    49,\n",
      "         6625,     6,  9597,    15,    49, 17690,     6,     8,   478,     5,\n",
      "          514, 37407,     4,   646,  3388,   510,   742,  8976,    59,   158,\n",
      "          107,   536,    38,    56,    10, 28242,   676,     4,    85,    21,\n",
      "          727,   207,    49,  7684,    53,    51,   478,     5,   514,  6745,\n",
      "            8,  5601,     4,   252,    56,   117,  1746,    53,    51,   818,\n",
      "         2075,   162,   160,     5,   921,     4,     2,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1]), 'attention_mask_ctx': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'input_ids_full': tensor([    0,  7164,  1000,    35,    38,  1299,  2181,    77,    38,    21,\n",
      "         1428,   184,    65,   363,     8,    10,   621,  1381,     7,  3598,\n",
      "           88,   127,  6625,     6,     8,   399,    75,   192,   162,     4,\n",
      "           38, 11006, 10916,     8,    51,  3514, 13539,   124,    88,    49,\n",
      "         6625,     6,  9597,    15,    49, 17690,     6,     8,   478,     5,\n",
      "          514, 37407,     4,   646,  3388,   510,   742,  8976,    59,   158,\n",
      "          107,   536,    38,    56,    10, 28242,   676,     4,    85,    21,\n",
      "          727,   207,    49,  7684,    53,    51,   478,     5,   514,  6745,\n",
      "            8,  5601,     4,   252,    56,   117,  1746,    53,    51,   818,\n",
      "         2075,   162,   160,     5,   921,     4,   646,  3388,   510,   742,\n",
      "          248,  4186,    35,  6553,    47,  6297,   143,  1746,   116,     2,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1]), 'attention_mask_full': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor(3.6400), 'emotion_labels': tensor(18)}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e3cacd3438b472a8a0e7d48c2709b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5076 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "031a395899154866814cf3cdd8781d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/627 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d8f8fb1e054441a7c2d466808fda43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/669 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: ./empathy_dataset/hf_tokenized_multiview\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "RAW_ED_PATH = \"empathy_dataset/hf_raw_with_emo\"  # 그대로 사용\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "MAX_LEN = 256\n",
    "\n",
    "# 1) raw ED 로드\n",
    "ds = load_from_disk(RAW_ED_PATH)\n",
    "print(ds)\n",
    "\n",
    "# 2) multi-view 텍스트 생성\n",
    "def build_multiview(example):\n",
    "    ctx = example[\"context\"]\n",
    "    rsp = example[\"response\"]\n",
    "    example[\"input_ctx\"] = \"CTX: \" + ctx\n",
    "    example[\"input_full\"] = f\"CTX: {ctx} [SEP] RSP: {rsp}\"\n",
    "    return example\n",
    "\n",
    "ds_mv = ds.map(build_multiview)\n",
    "\n",
    "# 3) 토크나이즈\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_views(batch):\n",
    "    enc_ctx = tokenizer(\n",
    "        batch[\"input_ctx\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "    enc_full = tokenizer(\n",
    "        batch[\"input_full\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_ids_ctx\": enc_ctx[\"input_ids\"],\n",
    "        \"attention_mask_ctx\": enc_ctx[\"attention_mask\"],\n",
    "        \"input_ids_full\": enc_full[\"input_ids\"],\n",
    "        \"attention_mask_full\": enc_full[\"attention_mask\"],\n",
    "        \"labels\": batch[\"label\"],\n",
    "        \"emotion_labels\": batch[\"emotion_id\"],\n",
    "    }\n",
    "\n",
    "cols_to_remove = ds_mv[\"train\"].column_names\n",
    "ds_tokenized = ds_mv.map(\n",
    "    tokenize_views,\n",
    "    batched=True,\n",
    "    remove_columns=cols_to_remove,\n",
    ")\n",
    "\n",
    "# 4) torch 포맷\n",
    "for split in ds_tokenized.keys():\n",
    "    ds_tokenized[split].set_format(\n",
    "        type=\"torch\",\n",
    "        columns=[\n",
    "            \"input_ids_ctx\", \"attention_mask_ctx\",\n",
    "            \"input_ids_full\", \"attention_mask_full\",\n",
    "            \"labels\", \"emotion_labels\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "print(ds_tokenized)\n",
    "print(ds_tokenized[\"train\"][0])\n",
    "\n",
    "# 5) 이번에는 꼭 저장!\n",
    "SAVE_MULTI_PATH = \"./empathy_dataset/hf_tokenized_multiview\"\n",
    "ds_tokenized.save_to_disk(SAVE_MULTI_PATH)\n",
    "print(\"Saved to:\", SAVE_MULTI_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc692e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d144adf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
